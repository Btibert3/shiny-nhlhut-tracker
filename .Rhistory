salaries = data.frame()
## loop and parse the data
for (i in PAGES) {
## build the URL and get the page
URL = sprintf(BASE_URL, i)
resp = URL %>% read_html() %>% html_nodes(xpath=TABLE_XPATH)
stats = resp[[1]] %>% html_table()
## make all strings to avoid type issues and bind the data
stats2 = mutate_all(stats, funs(as.character(.)))
salaries = bind_rows(salaries, stats2)
# salaries = mutate_all(salaries, funs(as.character(.)))
## cleanup
rm(URL, resp, stats, stats2)
## status
cat("finished ", i, "\n")
} #endfor
glimpse(salaries)
## save out the dataset
write_csv(salaries, "~/Downloads/salaries1415.csv", na="")
devtools::install_github("btibert3/ripeds")
library(ipeds)
devtools::install_github("jbryer/ipeds")
library(ipeds)
data(surveys)
hd = getIPEDSSurvey("hd", "2016")
hd = getIPEDSSurvey("HD", "2016")
ic = getIPEDSSurvey("IC", 2016)
sfa = getIPEDSSurvey("SFA", "1617")
hd = getIPEDSSurvey("HD", "2015")
ic = getIPEDSSurvey("IC", 2015)
sfa = getIPEDSSurvey("SFA", "1516")
adm = getIPEDSSurvey("ADM", 2015)
library(dplyr)
schools = inner_join(hd, ic)
schools = left_join(schools, sfa)
schools = inner_join(hd, ic)
schools = inner_join(schools, sfa)
schools = inner_join(schools, adm)
glimpse(schools)
schools_final = schools %>%
select(unitid:obereg, sector, deggrant, carnegie, landgrnt,
longitud:latitude, applfeeu, igrnt_p, igrnt_a, loan_p,
loan_a, applcn, applcnm, applcnw, admssn, admssnm, admssnw,
enrlt, enrlm, enrlw, satpct, actpct, satvr25, satvr75,
satmt25, satmt75, satwr25, satwr75)
save(schools_final, file="~/Downloads/admissions15.Rdata")
readr::write_csv(schools_final, "~/Downloads/admissions15.csv", na="")
x = read.csv("myfile.csv")
list.files()
setwd("~/github/analyze-attendee-reports")
list.files()
setwd("figs/")
list.files()
setwd("figs")
setwd("..")
setwd("figs")
list.files()
library(cwl)
setwd("~/github")
source("scrape.R")
2+2
6/3
x = 2+2
x
class(x)
x = 2+2
y = read.csv("test.csv")
class(y)
class(x)
z = TRUE
class(z)
z = list(a=1, b="ABC", c=TRUE)
class(z)
z
z = list(a=1, b="ABC", c=TRUE, d=data.frame(a=1:5, b=LETTERS[1"5"]))
z = list(a=1, b="ABC", c=TRUE, d=data.frame(a=1:5, b=LETTERS[5"]))
)
z = list(a=1, b="ABC", c=TRUE, d=data.frame(a=1:5, b=LETTERS[5]))
class(z)
z
mydat = z$d
class(mydat)
z
x = c("a", "B", "c")
x
x = as.factor(x)
x
gender = c("m", "f", "m", "m")
gender
gender2 = as.factor(gender)
gender2
2+2
options(stringsAsFactors = FALSE)
x = function(a=b, c=1) {
return(paste0(a, b))
}
x
x()
x = function(a=b, c=1) {
return(paste0(a, c))
}
x()
x = function(a=b, c=1) {
return(paste0(a, c))
}
x()
x = function(a="b", c=1) {
return(paste0(a, c))
}
x()
x = function(a="b", c=1) {
return(paste(a, c, sep=""))
}
x()
2+2
## load packages
library(cwl)
library(backyard)
detach("package:backyard", unload = TRUE)
## load packages
detach("package:cwl", unload = TRUE)
library(cwl)
detach("package:cwl", unload = TRUE)
## load packages
library(cwl)
options(stringsAsFactors = FALSE)
## load packages
library(dplyr)
options(stringsAsFactors = FALSE)
## load packages
library(dplyr)
library(cwl)
help("connect_redshift")
help('help')
??help
?help
??connect
library(assertthat)
detach("package:assertthat", unload = TRUE)
library(bindr)
detach("package:bindr", unload = TRUE)
R.Version()
## load packages
library(cwl)
x = function(a=1, b=2) {return(a+b)}
x()
x(a=5)
## connect to redshift
rs_db = connect_redshift()
?db_list_tables
class(rs_db)
db_list_tables(rs_db)
dbListTables(rs_db)
dbListFields(rs_db, "dim_user")
?dbListTables
dbListFields(rs_db, "dim_user")
devtools::install_github("r-lib/usethis")
library(usethis)
library(usethis)
?create_package
create_package("et")
install.packages("rsample")
options(stringsAsFactors = FALSE)
## load the packages
library(tidyverse)
library(ipeds)
install.packages("tidymodels")
library(tidymodels)
## get the dataset
data("surveys")
## get the schools
schools = getIPEDSSurvey("HD", "2017")
library(dplyr)
glimpse(schools)
schools = schools %>% select(unitid:obereg, sector, hdegofr1, carnegie)
glimpse(schools)
schools = schools %>%
select(unitid:obereg, sector, hdegofr1, carnegie) %>%
filter(sector %in% c(1:2))
glimpse(schools)
schools = schools %>%
filter(sector %in% c(1:2)) %>%
filter(pset4flg == 1) %>%
select(unitid:obereg, sector, hdegofr1, carnegie)
## get the schools
schools = getIPEDSSurvey("HD", "2017")
colnames(schools)
schools = schools %>%
filter(sector %in% c(1:2)) %>%
filter(pset4flg == 1) %>%
select(unitid:obereg, sector, hdegofr1, carnegie)
## get the schools
hd17 = getIPEDSSurvey("HD", "2017")
schools = hd17 %>%
filter(sector %in% c(1:2)) %>%
filter(pset4flg == 1) %>%
select(unitid:obereg, sector, hdegofr1, carnegie)
glimpse(schools)
## load the packages
library(caTools)
library(caret)
install.packages("tidymodels")
install.packages("tidymodels")
library(caret)
install.packages("caret")
library(caret)
colnames(schools)
targets = schools %>%
filter(obereg != 9) %>%
sample_n(size=15)
library(dplyr)
targets = schools %>%
filter(obereg != 9) %>%
sample_n(size=15)
targets = schools %>%
filter(obereg != 9) %>%
sample_n(size=150)
glimpse(targets)
library(skimr)
install.packages("skimr")
library(skimr)
## look at the data
skim(targets)
library(janitor)
## look at the data
targets %>% tabyl(obereg, sector)
View(targets)
ufa1 = load("~/Downloads/ufa1.Rdata")
rm(ufa1)
load("~/Downloads/ufa1.Rdata")
library(dplyr)
glimpse(ufa1)
options(stringsAsFactors = FALSE)
## bring in the data
library(jsonlite)
## steam dataset
x = read_json("~/Downloads/australian_users_items.json")
x = rjson::fromJSON("~/Downloads/australian_users_items.json")
x = rjson::fromJSON(file = "~/Downloads/australian_users_items.json")
## steam dataset
x = read_json("~/Downloads/renttherunway_final_data.json")
library(purrr)
??transmute
library(dplyr)
?transmute
x = matrix(nrow=3, ncol=3, data=2)
x
library(arules)
install.packages('arules')
library(arules)
?apriori
options(stringsAsFactors = FALSE)
## load  (and maybe install) the necessary packages w/o install interface using github, not CRAN
install.packages("devtools")
devtools::install_github("tidyverse/readxl")
library(readxl)
## get the raw datafile -- downloads to local directory getwd()
URL = "https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx"
download.file(URL, destfile = "retail.xlsx")
raw = read_excel("retail.xlsx")
## quick glimpse
class(raw)
colnames(raw)
dim(raw)
head(raw)
tail(raw)
## I think its easier to work with lowercase
colnames(raw) = tolower(colnames(raw))
## detail review with one command from an extension library
install.packages("dplyr"); library(devtools);
library(devtools)
install_github("ropensci/skimr")
library(skimr)
install_github("sfirke/janitor")
library(janitor)
skim(raw)   ## <---- how easy is this and way better than summary(raw)
## ok, way too many missing customerids, remove to start to get closer to our transaction dataset
dat = raw %>% filter(!is.na(customerid))
## lets aggregate data by customer, stockcode, and sum qnty for example sake
dim(dat)
## ok, way too many missing customerids, remove to start to get closer to our transaction dataset
dat = raw %>% filter(!is.na(customerid))
## ok, way too many missing customerids, remove to start to get closer to our transaction dataset
dat = raw %>% filter(!is.na(customerid))
colnames(raw)
## ok, way too many missing customerids, remove to start to get closer to our transaction dataset
dat = raw %>% filter(!is.na(customerid))
skim(raw)
## ok, way too many missing customerids, remove to start to get closer to our transaction dataset
dat = raw %>% filter(!is.na(customerid))
class(raw)
## ok, way too many missing customerids, remove to start to get closer to our transaction dataset
dat = as.data.frame(raw) %>% filter(!is.na(customerid))
raw = as.data.frame(raw)
## ok, way too many missing customerids, remove to start to get closer to our transaction dataset
dat = raw %>% filter(!is.na(customerid))
options(stringsAsFactors = FALSE)
## load  (and maybe install) the necessary packages w/o install interface using github, not CRAN
install.packages("devtools")
devtools::install_github("tidyverse/readxl")
library(readxl)
## get the raw datafile -- downloads to local directory getwd()
URL = "https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx"
download.file(URL, destfile = "retail.xlsx")
install.packages("devtools")
library(readxl)
## get the raw datafile -- downloads to local directory getwd()
URL = "https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx"
download.file(URL, destfile = "retail.xlsx")
raw = read_excel("retail.xlsx")
## quick glimpse
class(raw)
colnames(raw)
dim(raw)
head(raw)
tail(raw)
## I think its easier to work with lowercase
colnames(raw) = tolower(colnames(raw))
## detail review with one command from an extension library
install.packages("dplyr"); library(dplyr);
library(devtools)
install_github("ropensci/skimr")
library(skimr)
install_github("sfirke/janitor")
library(janitor)
skim(raw)   ## <---- how easy is this and way better than summary(raw)
## ok, way too many missing customerids, remove to start to get closer to our transaction dataset
dat = raw %>% filter(!is.na(customerid))
## lets aggregate data by customer, stockcode, and sum qnty for example sake
dim(dat)
dat2 = dat %>% select(customerid, stockcode)
dim(dat2)
## get the data mining packages
install_github("mhahsler/recommenderlab")
library(recommenderlab)
## More effecient  - write out and read back in
## Example 4: https://www.rdocumentation.org/packages/arules/versions/1.6-1/topics/transactions-class
head(dat2)
write.csv(dat2, file = "retail-trans.csv", na="")
trans = read.transactions("retail-trans.csv", format = "single", sep=",", cols=1:2)
class(trans)
## split the transactions into training/test
easplit = evaluationScheme(trans, method="split", train=.75)
## split the transactions into training/test for the rating matrix
trans_mat = as(trans, "binaryRatingMatrix")
easplit = evaluationScheme(trans, method="split", train=.75)
## split the transactions into training/test for the rating matrix
trans_mat = as(trans, "ratingMatrix")
options(stringsAsFactors = FALSE)
library(rpart)
?data
data()
data(sleep)
str(sleep)
data(mtcars)
str(mtcars)
mod = rpart(mpg ~ ., data=mtcars)
rsq.rpart(mod)
tmp <- printcp(mod)
tmp
class(tmp)
names(tmp)
tmp[,3]
rsq.val <- 1-tmp[,c(3,4)]
rsq.val
library(tidyverse)
?slice
library(tidyverse)
raw = read_csv("~/Downloads/example-trans - Sheet1.csv")
raw = read_csv("~/Downloads/example-trans - Sheet1.csv", col_names = FALSE)
colnames(raw) = c("id", "items")
raw
raw = read_csv("~/Downloads/example-trans - Sheet1.csv")
raw2 = raw %>% separate(items, sep="|", into=paste0("item", 1:10))
raw2
raw
raw2 = raw %>% separate(items, sep="|", into=paste0("item", 1:10))
raw2
options(stringsAsFactors = FALSE)
raw = read_csv("~/Downloads/example-trans - Sheet1.csv")
raw2 = raw %>% separate(items, sep="|", into=paste0("item", 1:10))
raw2
raw = read_csv("~/Downloads/example-trans - Sheet1.csv")
raw2 = raw %>% separate(items, sep="|", into=paste0("item", 1:10))
raw2
raw2 = raw %>% separate(items, sep="~", into=paste0("item", 1:10))
raw2
trans = raw2 %>% gather(basket, item, -trans)
trans
trans = raw2 %>% gather(basket, item, -trans) %>% arrange(trans)
trans
trans = raw2 %>% gather(basket, item, -trans, na.rm=TRUE) %>% arrange(trans)
trans
trans = raw2 %>% gather(basket, item, -trans, na.rm=TRUE) %>% arrange(trans) %>% select(-basket)
trans
library(trans)
library(arules)
basket = as(split(trans[,"item"], trans[,"trans"]), "transactions")
trans
trans = as.data.frame(trans)
basket = as(split(trans[,"item"], trans[,"trans"]), "transactions")
rules = apriori(basket)
summary(rules)
rules
## calculate several measures
basket_stats <- interestMeasure(rules, c("support", "confidence", "lift"),
transactions = basket)
inspect(head(basket_stats))
head(basket_stats)
basket
rules
basket_stats <- interestMeasure(basket, c("support", "confidence", "lift"),
transactions = basket)
basket_stats <- interestMeasure(rules, c("support", "confidence", "lift"),
transactions = basket)
basket_stats <- interestMeasure(rules, c("support", "confidence", "lift"),
transactions = basket, reuse = FALSE)
quality(rules)
options(stringsAsFactors = FALSE)
library(tidyverse)
raw = read_csv("~/Downloads/example-trans - Sheet1.csv")
raw2 = raw %>% separate(items, sep="~", into=paste0("item", 1:10))
trans = raw2 %>% gather(basket, item, -trans, na.rm=TRUE) %>% arrange(trans) %>% select(-basket)
library(arules)
trans = as.data.frame(trans)
basket = as(split(trans[,"item"], trans[,"trans"]), "transactions")
rules = apriori(basket)
quality(rules)
inspect(rules)
inspect(rules)
quality(rules)
rulesdf = inspect(rules)
class(rulesdf)
rulesdf %>% arrange(desc(lift))
rulesdf %>% arrange(desc(lift))
head(rulesdf)
colnames(rulesdf)
rulesdf = inspect(rules) %>% select(lhs, rhs:count)
rulesdf %>% arrange(desc(lift))
rulesdf
write_csv(rulesdf, "~/Downloads/exam-rules.csv", na="")
options(stringsAsFactors = FALSE)
remotes::install_github("rstudio/gt")
library(gt)
library(gt)
vignette(package="gt")
remotes::install_github("rstudio/gt", build = TRUE,
build_opts = c("--no-resave-data", "--no-manual"))
install.packages("textTinyR")
library(textTinyR)
options(stringsAsFactors = FALSE)
## load the packages
library(textTinyR)
options(stringsAsFactors = FALSE)
## load the packages
library(dplyr)
## load the packages
library(readr)
url = "https://raw.githubusercontent.com/jbryer/ipeds/master/data/surveys.csv"
surveys = read_csv(url)
library(ipeds)
data(surveys)
readr::write_csv(surveys, "~/Downloads/surveys.csv", na="")
install.packages("atom-language-r")
install.packages("languageserver")
install.packages('IRkernel')  # Don’t forget step 2/2!
IRkernel::installspec()
install.packages("jupyter-client")
install.packages('IRkernel')
IRkernel::installspec()
IRkernel::installspec(user = FALSE)
options(stringsAsFactors = FALSE)
## load the libraries
library(cwl)
## connect to redshift
ch = connect_redshift()
## get the data for chat
SQL = "
SELECT    a.*,
b.user_attendee_type,
b.email_address,c.external_id as acct_extid
FROM      fact_activity_chat a,
dim_user b,
dim_account c
WHERE     a.message NOT IN ('JOINED', 'LEFT')
AND       a.dim_user_id = b.id
AND       b.dim_account_id = c.id
AND       TRUNC(activity_date) >= '2018-01-01'
"
chat_raw = dbGetQuery(ch, SQL)
remotes::install_github("mkearney/rreddit")
options(stringsAsFactors = FALSE)
## get up to 100,000 of the most recent posts made to /r/dataisbeautiful
d <- get_r_reddit("ApplyingToCollege", n = 1000)
## get up to 100,000 of the most recent posts made to /r/dataisbeautiful
d <- rreddit::get_r_reddit("ApplyingToCollege", n = 1000)
glimpse(d)
dplyr::glimpse(d)
View(d)
glimpse(d)
dplyr::glimpse(d)
library(dplyr)
d %>% group_by(id) %>% count(sort = TRUE)
x = 1545086743630
as.Date(x)
as.Date(x, origin = "1970-01-01")
setwd("~/github/shiny-nhlhut-tracker")
shiny::run("hut-price-tracker/app/")
shiny::rrunApp("hut-price-tracker/app/")
shiny::runApp("hut-price-tracker/app/")
?shiny::runApp
shiny::runApp("hut-price-tracker/app/", port=3838)
